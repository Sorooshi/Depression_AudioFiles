# -*- coding: utf-8 -*-
"""DeepAR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q5UD-MA4XhKpmNVLRxca3YH4YIeMAfGF

# Installations
"""

!pip install torch
!pip install pytorch_forecasting
!pip install dtaidistance

"""# Main part

## Loading df
"""

import os
import numpy as np
import pandas as pd
import librosa as lb
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

#Global Sample Rate
# SR = 22000
SR = 8000

SEED = 42
np.random.seed(SEED)

DRIVE_PATH = 'drive/MyDrive/psychiatric.disorders.ML'

# participants_info
# participants = pd.read_excel("../Datasets/psychiatric_disorders_data/PsychiatricDiscourse_participant_data.xlsx")

participants = pd.read_excel(os.path.join(DRIVE_PATH, 'PsychiatricDiscourse_participant.data.xlsx'))

participants

# depression_only
depression_only = participants.loc[
    (participants['thought.disorder.symptoms'] == 0.) &
    (participants['depression.symptoms'] != 0.)
]

control_group = participants.loc[
    (participants['depression.symptoms'] == 0.) &
    (participants['thought.disorder.symptoms'] == 0.)
]

df = pd.concat([depression_only, control_group])

def get_patient_audio(row, data_folder=os.path.join(DRIVE_PATH, 'wav files'), return_uncomplete=False):
    key = row.ID
    audio_files = []
    for filename in os.listdir(data_folder):
        if filename.find(key) != -1:
            audio_files.append(filename)
    return audio_files

df['audio'] = df.apply(get_patient_audio, axis=1)

# exclude patients with num of recordings other than 3
df = df[df.audio.apply(len) == 3]

os.path.join(DRIVE_PATH, 'wav files')

task_mapping = {
    'narrative': ['sportsman', 'adventure', 'winterday'], 
    'story': ['present', 'trip', 'party'], 
    'instruction': ['chair', 'table', 'bench']
}

def get_domain_audio(row, domain):
    files = []
    for topic in task_mapping[domain]:
        for file_name in row.audio:
            if file_name.find(topic) != -1:
                files.append(file_name)
                
    if len(files) > 1:
        print(files)
    # assert len(files) < 2
    return files[0] if len(files) else None
    
    
    
for domain in task_mapping:
    df[f'audio.{domain}'] = df.apply(get_domain_audio, axis=1, domain=domain)

df.head()

df['depression.symptoms'].value_counts()

df[['audio.narrative', 'audio.story', 'audio.instruction']].dropna()

#80% training data and 20% test data. Split so that test data will include all types of depression severity

from sklearn.model_selection import StratifiedShuffleSplit

sss = StratifiedShuffleSplit(n_splits=1, test_size = 0.2, train_size = 0.8, random_state = 42)

for (train_index, test_index) in sss.split(df, df['depression.symptoms']):
  train_df = df.iloc[train_index]
  test_df = df.iloc[test_index]

train_df.head()

test_df.shape

"""## Data and train

### Packages and functions
"""

# from tqdm import tqdm
from tqdm.auto import tqdm
import pytorch_forecasting as ptf
import torch
from pytorch_forecasting import DeepAR, TimeSeriesDataSet
from pytorch_forecasting.data import NaNLabelEncoder
import lightning.pytorch as pl
from lightning.pytorch.callbacks import EarlyStopping
from pytorch_forecasting.metrics import RMSE, MultivariateNormalDistributionLoss, DistributionLoss
from dtaidistance import dtw
from itertools import product
import sklearn.metrics as skm 


import warnings
warnings.filterwarnings('ignore')


pl.seed_everything(42)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

def concat_df(new_df_narr, new_df_story, new_df_instr):
  un_narr = new_df_narr['group'].unique()
  un_story = new_df_story['group'].unique()
  un_instr = new_df_instr['group'].unique()

  new_df = pd.DataFrame(columns = ['narrative', 'story', 'instruction', 'time_idx', 'group'])
  time_idx = new_df_narr.loc[new_df_narr['group'] == un_narr[0]]['time_idx']

  for group in np.unique(np.concatenate((un_narr, un_story, un_instr), 0)):
    flag_narr = group in un_narr
    flag_story = group in un_story
    flag_instr = group in un_instr
    tmp_df = pd.DataFrame()

    if flag_narr:
      tmp_df['narrative'] = new_df_narr.loc[new_df_narr['group'] == group]['observations'].values
    else:
      tmp_df['narrative'] = new_df_narr.loc[new_df_narr['group'] == '00' + group[2:]]['observations'].values

    if flag_story:
      tmp_df['story'] = new_df_story.loc[new_df_story['group'] == group]['observations'].values
    else:
      tmp_df['story'] = new_df_story.loc[new_df_story['group'] == '00' + group[2:]]['observations'].values

    if flag_instr:
      tmp_df['instruction'] = new_df_instr.loc[new_df_instr['group'] == group]['observations'].values
    else:
      tmp_df['instruction'] = new_df_instr.loc[new_df_instr['group'] == '00' + group[2:]]['observations'].values

    tmp_df['time_idx'] = time_idx
    tmp_df['group'] = group
    new_df = pd.concat([new_df, tmp_df], axis=0, ignore_index=True)
  
  new_df['time_idx'] = new_df['time_idx'].astype(int)
  return new_df

def cut_recordings(data, audio_dur, cutoff_len, min_len = 0):
  res = []
  cnt = 1
  res.append(data[:int(cutoff_len * SR)])
  audio_dur -= cutoff_len

  while(audio_dur > min_len and cnt<10):
    res.append(data[int(cutoff_len * SR)*cnt:int(cutoff_len * SR)*(cnt+1)])
    audio_dur -= cutoff_len
    cnt += 1

  return res


def pad_ts(data, max_dur):
  for i in range(len(data)):
      if (len(data[i]) < max_dur):
        data[i] = np.pad(data[i], (max_dur - len(data[i]), 0), 'constant', constant_values=(0,))

  return data


def load_and_preprocess(files, data_folder, cutoff_len = 0, min_len=0):
  audio_ts = []

  for filename in files:
      signal, sr = lb.load(os.path.join(data_folder, filename), sr=SR)
      signal, _ = lb.effects.trim(signal, top_db=40)
      audio_ts.append(signal)


  for i in range(len(audio_ts)):
    audio_dur = len(audio_ts[i]) / SR

    audio_ts[i] = cut_recordings(data = audio_ts[i], audio_dur = audio_dur, 
                                 cutoff_len = cutoff_len, min_len = min_len)
    audio_ts[i] = pad_ts(audio_ts[i], cutoff_len*SR)

  upd_df = pd.DataFrame(columns=[f'observations', 'time_idx', 'group'])
  audio_len = len(audio_ts[0][0])


  for i in range(len(audio_ts)):
    for j in range(len(audio_ts[i])):
      if j < 10:
        tmp_df = pd.DataFrame({'observations':audio_ts[i][j], 'time_idx' : np.arange(audio_len), 'group':[f'0{j}_' + files.iloc[i]] * audio_len})
      else: 
        tmp_df = pd.DataFrame({'observations':audio_ts[i][j], 'time_idx' : np.arange(audio_len), 'group':[f'{j}_' + files.iloc[i]] * audio_len})

      upd_df = pd.concat([upd_df, tmp_df], axis=0, ignore_index=True)

  upd_df['time_idx'] = upd_df['time_idx'].astype(int)

  return upd_df


def create_timeSeriesDataSet(df, encoder_len = 60, prediction_len = 60):
  # Replace "." with "_"
  df.columns = [col.replace(".", "_") for col in df.columns]

  # Define the TimeSeriesDataSet object
  training_cutoff = df["time_idx"].max() - prediction_len

  training = TimeSeriesDataSet(
      data=df.loc[lambda x: x.time_idx <= training_cutoff],
      time_idx="time_idx",
      target = ['narrative', 'story', 'instruction'],
      group_ids= ["group"],
      max_encoder_length=encoder_len,
      max_prediction_length=prediction_len,
      time_varying_unknown_reals=['narrative', 'story', 'instruction'],
      # time_varying_known_reals=["time_idx"], #for tft
  )

  validation = TimeSeriesDataSet.from_dataset(training, df, min_prediction_idx=training_cutoff+1)

  return (training, validation)


def create_timeSeriesDataSet_test(df, encoder_len = 60, prediction_len = 60):
  # Replace "." with "_"
  df.columns = [col.replace(".", "_") for col in df.columns]

  # Define the TimeSeriesDataSet object
  training_cutoff = df["time_idx"].max() - prediction_len

  test = TimeSeriesDataSet(
      data=df,
      time_idx="time_idx",
      target = ['narrative', 'story', 'instruction'],
      group_ids= ["group"],
      max_encoder_length=encoder_len,
      max_prediction_length=prediction_len,
      time_varying_unknown_reals=['narrative', 'story', 'instruction'],
      # time_varying_known_reals=["time_idx"], # for tft
      min_prediction_idx=training_cutoff+1,
  )

  return test

def classify_obs(pid, pred, df, data_folder, size_of_pred, audio_ts, stimuli_type):
  min_dist = float('inf')
  most_similar = None

  for filename in df[f'audio.{stimuli_type}']:
    distance_dtw = dtw.distance_fast(pred, audio_ts[filename][-size_of_pred:].astype(np.double))

    if distance_dtw < min_dist:
      min_dist = distance_dtw
      most_similar = filename
    
  return (pid, df[df[f'audio.{stimuli_type}'] == most_similar]['depression.symptoms'].iloc[0], min_dist)

"""### Training the model"""

BASE_LEN = 83
BATCH_SIZE = 2
LR = 1e-3
HIDDEN_SIZE = 10
RNN_LAYERS = 2
EPOCHS = 2
LIMIT_TRAIN_BATCHES = 2000

# Read first 4 observations
files = train_df['audio.narrative'][:4]
data_folder=os.path.join(DRIVE_PATH, 'wav files')
new_df = load_and_preprocess(files, data_folder, cutoff_len = BASE_LEN, min_len = 24)

files = train_df['audio.story'][:4]
new_df['observations_story'] = load_and_preprocess(files, data_folder, cutoff_len = BASE_LEN, min_len = 24)['observations']

files = train_df['audio.instruction'][:4]
new_df['observations_instruction'] = load_and_preprocess(files, data_folder, cutoff_len = BASE_LEN, min_len = 24)['observations']

train_val = create_timeSeriesDataSet(new_df)

train_dataloader = train_val[0].to_dataloader(
    train=True, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
)
# DeepAR model
net = DeepAR.from_dataset(
    train_val[0],
    learning_rate=1e-3,
    hidden_size=10,
    rnn_layers=2,
    optimizer="AdamW",
)
net.to(device)

early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=1e-4, patience=5, verbose=False, mode="min")

# Fit for the rest observations
for i in tqdm(range(0,len(train_df['audio.narrative']), 4)):

  files = train_df['audio.narrative'][i:i+4]
  data_folder=os.path.join(DRIVE_PATH, 'wav files')
  new_df = load_and_preprocess(files, data_folder)
  files = train_df['audio.story'][i:i+4]
  new_df['observations_story'] = load_and_preprocess(files, data_folder, min_len = 24)['observations']
  files = train_df['audio.instruction'][i:i+4]
  new_df['observations_instruction'] = load_and_preprocess(files, data_folder, min_len = 24)['observations']

  train_val = create_timeSeriesDataSet(new_df)

  train_dataloader = train_val[0].to_dataloader(
      train=True, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
  )

  val_dataloader = train_val[1].to_dataloader(
      train=False, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
  )

  trainer = pl.Trainer(
    max_epochs=2, #10 FOR NOW
    accelerator="gpu",
    enable_model_summary=False,
    gradient_clip_val=0.1,
    # limit_train_batches=2000,
    limit_train_batches=200,
    callbacks=[early_stop_callback],
    enable_checkpointing=True,
    check_val_every_n_epoch=1,
    # reload_dataloaders_every_n_epochs = 10
  ) 

  trainer.fit(
    net,
    train_dataloaders=train_dataloader,
    val_dataloaders=val_dataloader,
  )

"""### Fine-tuning"""

data_folder=os.path.join(DRIVE_PATH, 'wav files')

BATCH_SIZE = 8
LIMIT_TRAIN_BATCHES = 3000
EPOCHS = 2
BASE_LEN_f = 5

PREDICTION_LENGTH = 200
CONTEXT_LENGTH = 200 
LR = 5e-3

class loss_aa(DistributionLoss):
  def map_x_to_distribution(self, x):
    return torch.distributions.normal.Normal(x.mean(axis=2), torch.log(1 + torch.exp(x)).mean(axis=2))

def move_to_device(x):
  if type(x) == list:
    x = [i.to(device) for i in x]
  else:
    x = x.to(device)
  return x

def train_model(model, epochs):
  # Fit for the rest observations
  criterion = loss_aa()
  optimizer = torch.optim.AdamW(model.parameters(), lr=LR)

  for i in tqdm(range(0, len(train_df['audio.narrative']), 4), desc='Files loaded'):
    if (i+4 > len(train_df['audio.narrative'])):
      files_narr = train_df['audio.narrative'][i:]
      files_story = train_df['audio.story'][i:]
      files_instr = train_df['audio.instruction'][i:]
    else:
      files_narr = train_df['audio.narrative'][i:i+4]
      files_story = train_df['audio.story'][i:i+4]
      files_instr = train_df['audio.instruction'][i:i+4]

    data_folder=os.path.join(DRIVE_PATH, 'wav files')

    new_df_narr = load_and_preprocess(files_narr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
    new_df_story = load_and_preprocess(files_story, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
    new_df_instr = load_and_preprocess(files_instr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)

    new_df_narr['group'] = new_df_narr['group'].apply(lambda x : x[:9])
    new_df_story['group'] = new_df_story['group'].apply(lambda x : x[:9])
    new_df_instr['group'] = new_df_instr['group'].apply(lambda x : x[:9])

    new_df = concat_df(new_df_narr, new_df_story, new_df_instr)

    train_val = create_timeSeriesDataSet(new_df, prediction_len=PREDICTION_LENGTH, encoder_len=CONTEXT_LENGTH)

    train_dataloader = train_val[0].to_dataloader(
        train=True, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
    )

    val_dataloader = train_val[1].to_dataloader(
        train=False, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
    )
    for epoch in range(epochs):

      model.train()
      train_it = iter(train_dataloader)
      train_loss = 0
      for n_batches in range(LIMIT_TRAIN_BATCHES):
        batch = next(train_it)
        x = {k: move_to_device(v) for k, v in batch[0].items()}
        y = [v.to(device) for v in batch[1][0]]

        out = model.forward(x)
        loss = criterion(out['prediction'][0], y[0])
        loss += criterion(out['prediction'][1], y[1])
        loss += criterion(out['prediction'][2], y[2])
        loss = loss / 3

        train_loss += loss.detach()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


      model.eval()
      iteration = 0
      overall_loss = 0
      for val_batch in val_dataloader:
        x_val = {k: move_to_device(v) for k, v in val_batch[0].items()}
        y_val = [v.to(device) for v in val_batch[1][0]]
        
        out = model.forward(x_val)
        loss = criterion(out['prediction'][0], y_val[0])
        loss += criterion(out['prediction'][1], y_val[1])
        loss += criterion(out['prediction'][2], y_val[2])
        loss = loss / 3

        iteration += 1
        overall_loss += loss.detach()
      
      # print(f"Epoch {epoch} Train loss: ", round(float(train_loss / LIMIT_TRAIN_BATCHES), 4), "\tVal loss: ",  round(float(overall_loss / iteration), 4))
  return model

grid_v2 = { 'PREDICTION_CONTEXT_LENGTH': [[PREDICTION_LENGTH, CONTEXT_LENGTH]],
        'hidden_size ' : [32, 64, 80],
        'rnn_layers  ' : [2, 4, 6],
}

from itertools import product

deepar_paths = "/content/drive/MyDrive/Grid_search_thesis/DeepAR_models"

for i, params in tqdm(enumerate(product(*grid_v2.values())), total = len(list(product(*grid_v2.values()))), desc='Grid search progress'):
  if os.path.isfile(os.path.join(deepar_paths, f'Deepar_{i}.pt')):
    continue

  files_narr = train_df['audio.narrative'][:2]
  files_story = train_df['audio.story'][:2]
  files_instr = train_df['audio.instruction'][:2]

  new_df_narr = load_and_preprocess(files_narr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
  new_df_story = load_and_preprocess(files_story, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
  new_df_instr = load_and_preprocess(files_instr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)

  new_df_narr['group'] = new_df_narr['group'].apply(lambda x : x[:9])
  new_df_story['group'] = new_df_story['group'].apply(lambda x : x[:9])
  new_df_instr['group'] = new_df_instr['group'].apply(lambda x : x[:9])

  new_df = concat_df(new_df_narr, new_df_story, new_df_instr)

  train_val = create_timeSeriesDataSet(new_df, prediction_len=PREDICTION_LENGTH, encoder_len=CONTEXT_LENGTH)

  train_dataloader = train_val[0].to_dataloader(
      train=True, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
  )
  # DeepAR model
  net_deepar = DeepAR.from_dataset(
      train_val[0],
      learning_rate=LR,
      hidden_size=params[1],
      rnn_layers=params[2],
      optimizer="AdamW",
  )
  net_deepar.to(device)

  # net_deepar = train_model(net_deepar, epochs=2,)

  torch.save({'model_state_dict' : net_deepar.state_dict(),
            'hyperparameters' : net_deepar.hparams}, 
           os.path.join(deepar_paths, f'Deepar_{i}.pt'))

"""### Predictions"""

def classify_obs(pid, pred, df, data_folder, size_of_pred, audio_ts, stimuli_type):
  min_dist = float('inf')
  most_similar = None

  for filename in df[f'audio.{stimuli_type}']:
    distance_dtw = dtw.distance_fast(pred, audio_ts[filename][-size_of_pred:].astype(np.double))

    if distance_dtw < min_dist:
      min_dist = distance_dtw
      most_similar = filename
    
  return (pid, df[df[f'audio.{stimuli_type}'] == most_similar]['depression.symptoms'].iloc[0], min_dist)

data_folder=os.path.join(DRIVE_PATH, 'wav files')

BATCH_SIZE = 8
LIMIT_TRAIN_BATCHES = 3000
EPOCHS = 2
BASE_LEN_f = 5

PREDICTION_LENGTH = 200
CONTEXT_LENGTH = 200 
LR = 5e-3

deepar_paths = "/content/drive/MyDrive/Grid_search_thesis/DeepAR_models"
deepar_predictions_paths = '/content/drive/MyDrive/Grid_search_thesis/DeepAR_models/predictions'

files_narr = test_df['audio.narrative']
files_story = test_df['audio.story']
files_instr = test_df['audio.instruction']

new_df_narr = load_and_preprocess(files_narr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
new_df_story = load_and_preprocess(files_story, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
new_df_instr = load_and_preprocess(files_instr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)

new_df_narr['group'] = new_df_narr['group'].apply(lambda x : x[:9])
new_df_story['group'] = new_df_story['group'].apply(lambda x : x[:9])
new_df_instr['group'] = new_df_instr['group'].apply(lambda x : x[:9])

new_df = concat_df(new_df_narr, new_df_story, new_df_instr)

test = create_timeSeriesDataSet_test(new_df, prediction_len=PREDICTION_LENGTH, encoder_len=CONTEXT_LENGTH)

test_dataloader = test.to_dataloader(
    train=False, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
)

for i in tqdm(range(9)):
  checkpoint = torch.load(os.path.join(deepar_paths, f'Deepar_{i}.pt'), map_location=torch.device(device))

  # DeepAR model
  net_deepar = DeepAR.from_dataset(
      test,
      learning_rate=LR,
      hidden_size=checkpoint['hyperparameters']['hidden_size'],
      rnn_layers=checkpoint['hyperparameters']['rnn_layers'],
      optimizer="AdamW",
  )
  net_deepar.to(device)

  raw_predictions = net_deepar.predict(
    test_dataloader, mode="raw",
  )

  types = ['narrative', 'story', 'instruction']
  pred_df = pd.DataFrame()


  for st, stimuli_type in enumerate(types):

    predicted_classes = []
    size_of_pred = raw_predictions['prediction'][st].shape[1]
    groups = new_df['group'].unique()

    audio_ts = {}

    for filename in train_df[f'audio.{stimuli_type}']:
        signal, sr = lb.load(os.path.join(data_folder, filename), sr=SR)
        signal, _ = lb.effects.trim(signal, top_db=35)
        audio_ts[filename] = signal

    for m in range(raw_predictions['prediction'][st].shape[0]):
      tmp = classify_obs(pid = groups[m], 
                        pred = raw_predictions['prediction'][st][m].mean(axis=1).detach().cpu().numpy().astype(np.double),
                        df = train_df, 
                        data_folder = data_folder,
                        size_of_pred = size_of_pred,
                        audio_ts = audio_ts,
                        stimuli_type = stimuli_type)
      predicted_classes.append(tmp)


    pred_df_stimuli = pd.DataFrame(predicted_classes, columns=['id', f'pred_{stimuli_type}', f'min_dist_{stimuli_type}'])
    pred_df['id'] = pred_df_stimuli['id']
    pred_df[f'pred_{stimuli_type}'] = pred_df_stimuli[f'pred_{stimuli_type}']
    pred_df[f'min_dist_{stimuli_type}'] = pred_df_stimuli[f'min_dist_{stimuli_type}']
    
  pred_df['actual_severity'] = [test_df[test_df[f'ID'] == j[3:]]['depression.symptoms'].iloc[0] for j in pred_df['id']]
  pred_df.to_csv(os.path.join(deepar_predictions_paths,f'Deepar_{i}_pred_multiple.csv'))

"""### Choose best model """

deepar_paths = "/content/drive/MyDrive/Grid_search_thesis/DeepAR_models"

checkpoint = torch.load(os.path.join(deepar_paths, f'Deepar_{1}.pt'), map_location=torch.device(device))

# checkpoint['hyperparameters']

import sklearn.metrics as skm 

deepar_paths = "/content/drive/MyDrive/Grid_search_thesis/DeepAR_models"
deepar_predictions_paths = '/content/drive/MyDrive/Grid_search_thesis/DeepAR_models/predictions'

# stimuli_type = 'narrative'
# stimuli_type = 'story'
# stimuli_type = 'instruction'

best_score = 0
model_n = 0

for j in range(9):
  predictions = pd.read_csv(os.path.join(deepar_predictions_paths, f'Deepar_{j}_pred_multiple.csv'), index_col=0)
  predictions['id'] = predictions['id'].apply(lambda x: x[3:])

  # predictions = pd.read_csv(os.path.join(deepar_predictions_paths, f'Deepar_{1}_pred_multiple.csv'), index_col=0)
  # predictions['id'] = predictions['id'].apply(lambda x: x[3:])

  maj_voting_pred = predictions.groupby(['id']).agg(lambda x:x.value_counts().index[0])

  report_deepar = skm.classification_report(maj_voting_pred['actual_severity'], maj_voting_pred[['pred_narrative', 'pred_story', 'pred_instruction']].mode(axis=1)[0], output_dict=True)

  if report_deepar['weighted avg']['f1-score'] > best_score:
    model_n = j
    best_score = report_deepar['weighted avg']['f1-score']

print(model_n, best_score)

"""### CV"""

BATCH_SIZE = 8
LIMIT_TRAIN_BATCHES = 3000
EPOCHS = 2
BASE_LEN_f = 5

PREDICTION_LENGTH = 400
CONTEXT_LENGTH = 800 
LR = 3e-3

data_folder=os.path.join(DRIVE_PATH, 'wav files')

from sklearn.model_selection import StratifiedShuffleSplit

sss = StratifiedShuffleSplit(n_splits=10, test_size = 0.2, train_size = 0.8, random_state = 42)
cv_splits = {}


for i, (train_index, test_index) in enumerate(sss.split(df, df['depression.symptoms'])):
  cv_splits[i] = (train_index, test_index)

deepar_predictions_paths = '/content/drive/MyDrive/Grid_search_thesis/DeepAR_models/predictions'
deepar_cv = '/content/drive/MyDrive/Grid_search_thesis/DeepAR_models/cv'
deepar_paths = "/content/drive/MyDrive/Grid_search_thesis/DeepAR_models"

checkpoint = torch.load(os.path.join(deepar_paths, f'Deepar_{1}.pt'), map_location=torch.device(device))

for i, idx in enumerate(cv_splits.values()):
    # Obtain new train and test dataframes
    train_df = df.iloc[idx[0]]
    test_df = df.iloc[idx[1]]


    # Initialize the model
    data_folder=os.path.join(DRIVE_PATH, 'wav files')

    files_narr = train_df['audio.narrative'][:2]
    files_story = train_df['audio.story'][:2]
    files_instr = train_df['audio.instruction'][:2]

    new_df_narr = load_and_preprocess(files_narr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
    new_df_story = load_and_preprocess(files_story, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
    new_df_instr = load_and_preprocess(files_instr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)

    new_df_narr['group'] = new_df_narr['group'].apply(lambda x : x[:9])
    new_df_story['group'] = new_df_story['group'].apply(lambda x : x[:9])
    new_df_instr['group'] = new_df_instr['group'].apply(lambda x : x[:9])

    new_df = concat_df(new_df_narr, new_df_story, new_df_instr)

    train_val = create_timeSeriesDataSet(new_df, prediction_len=PREDICTION_LENGTH, encoder_len=CONTEXT_LENGTH)

    train_dataloader = train_val[0].to_dataloader(
        train=True, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
    )
    # DeepAR model
    net_deepar = DeepAR.from_dataset(
        train_val[0],
        learning_rate=LR,
        hidden_size=10,
        rnn_layers=2,
        optimizer="AdamW",
    )
    net_deepar.to(device)
    #Training
    # net_deepar = train_model(net_deepar, epochs=3)

    #Obtain predictions
    files_narr = test_df['audio.narrative']
    files_story = test_df['audio.story']
    files_instr = test_df['audio.instruction']

    new_df_narr = load_and_preprocess(files_narr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
    new_df_story = load_and_preprocess(files_story, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)
    new_df_instr = load_and_preprocess(files_instr, data_folder, cutoff_len = BASE_LEN_f, min_len = BASE_LEN_f)

    new_df_narr['group'] = new_df_narr['group'].apply(lambda x : x[:9])
    new_df_story['group'] = new_df_story['group'].apply(lambda x : x[:9])
    new_df_instr['group'] = new_df_instr['group'].apply(lambda x : x[:9])

    new_df = concat_df(new_df_narr, new_df_story, new_df_instr)

    test = create_timeSeriesDataSet_test(new_df, prediction_len=PREDICTION_LENGTH, encoder_len=CONTEXT_LENGTH)

    test_dataloader = test.to_dataloader(
        train=False, batch_size=BATCH_SIZE, num_workers=0, batch_sampler="synchronized"
    )

    raw_predictions = net_deepar.predict(
      test_dataloader, mode="raw",
    )

    types = ['narrative', 'story', 'instruction']
    pred_df = pd.DataFrame()


    for st, stimuli_type in enumerate(types):

      predicted_classes = []
      size_of_pred = raw_predictions['prediction'][st].shape[1]
      groups = new_df['group'].unique()

      audio_ts = {}

      for filename in train_df[f'audio.{stimuli_type}']:
          signal, sr = lb.load(os.path.join(data_folder, filename), sr=SR)
          signal, _ = lb.effects.trim(signal, top_db=35)
          audio_ts[filename] = signal

      for m in range(raw_predictions['prediction'][st].shape[0]):
        tmp = classify_obs(pid = groups[m], 
                          pred = raw_predictions['prediction'][st][m].mean(axis=1).detach().cpu().numpy().astype(np.double),
                          df = train_df, 
                          data_folder = data_folder,
                          size_of_pred = size_of_pred,
                          audio_ts = audio_ts,
                          stimuli_type = stimuli_type)
        predicted_classes.append(tmp)


      pred_df_stimuli = pd.DataFrame(predicted_classes, columns=['id', f'pred_{stimuli_type}', f'min_dist_{stimuli_type}'])
      pred_df['id'] = pred_df_stimuli['id']
      pred_df[f'pred_{stimuli_type}'] = pred_df_stimuli[f'pred_{stimuli_type}']
      pred_df[f'min_dist_{stimuli_type}'] = pred_df_stimuli[f'min_dist_{stimuli_type}']
      
    pred_df['actual_severity'] = [test_df[test_df[f'ID'] == j[3:]]['depression.symptoms'].iloc[0] for j in pred_df['id']]

    pred_df['id'] = pred_df['id'].apply(lambda x: x[3:])


    maj_voting_pred = pred_df.groupby(['id']).agg(lambda x:x.value_counts().index[0])


    deepar_pred = maj_voting_pred[['pred_narrative', 'pred_story', 'pred_instruction']].mode(axis=1)[0]

    deepar_final = pd.DataFrame([deepar_pred, maj_voting_pred['actual_severity']]).T

    deepar_final['id'] = deepar_final.index.values
    deepar_final = deepar_final.reset_index(drop=True)
    deepar_final.rename({0 : 'pred_severity'}, axis=1, inplace=True)
    deepar_final.to_csv(os.path.join(deepar_cv, f'deepar_fold_{i}_pred.csv'), index=False)

"""# **Playground**

## **Cut into multiple recordings**
"""

from tqdm import tqdm
import pytorch_forecasting as ptf
import torch
from pytorch_forecasting import DeepAR, TimeSeriesDataSet
from pytorch_forecasting.data import NaNLabelEncoder
import lightning.pytorch as pl
from lightning.pytorch.callbacks import EarlyStopping

pl.seed_everything(42)

files = df['audio.narrative'][:4]
data_folder=os.path.join(DRIVE_PATH, 'wav files')

audio_ts = []

for _, filename in tqdm(files.items(), total=len(files)):
    signal, sr = lb.load(os.path.join(data_folder, filename), sr=SR)
    audio_ts.append(signal)

"""Based on the statistics described median recording length is around 80 seconds. Let's take this as the length to which we want to bring all of other. I'm not choosing the lowest length due to the possibility of newly obtained recordings to be uninformative and become useless after splitting into that many parts. Whereas with the median recording length we can both pad the shorter recodings without losing any information and cut the longer ones. The reason for not choosing the mean recording length is the skew of their distribution. From the plots of recordings durations we may observe the presence of few anomalously long recordings, which could have had a great effect on our mean value."""

def cut_recordings(data, audio_dur, cutoff_len):
  res = []
  cnt = 1
  res.append(data[:int(cutoff_len * SR)])
  audio_dur -= cutoff_len

  while(audio_dur > 0): # Here, 0 is to be changed to some other value as some recordings may be cut into short audios
    res.append(data[int(cutoff_len * SR)*cnt:int(cutoff_len * SR)*(cnt+1)])
    audio_dur -= cutoff_len
    cnt += 1

  return res


def pad_ts(data, max_dur):
  for i in range(len(data)):
      if (len(data[i]) < max_dur):
        data[i] = np.pad(data[i], (max_dur - len(data[i]), 0), 'constant', constant_values=(0,))

  return data


# base_len = df_lengths.describe().loc['50%']
BASE_LEN = 83

for i in range(len(audio_ts)):
  audio_dur = len(audio_ts[i]) / SR
  audio_ts[i] = cut_recordings(audio_ts[i], audio_dur, BASE_LEN)

  audio_ts[i] = pad_ts(audio_ts[i], BASE_LEN*SR)

upd_df = pd.DataFrame(columns=['values', 'time_idx', 'group'])
audio_len = len(audio_ts[0][0])


for i in range(len(audio_ts)):
  for j in range(len(audio_ts[i])):
    tmp_df = pd.DataFrame({'values':audio_ts[i][j], 'time_idx' : np.arange(audio_len), 'group':[f'{j}_' + df['audio.narrative'].iloc[i]] * audio_len})

    upd_df = pd.concat([upd_df, tmp_df], axis=0, ignore_index=True)

upd_df['time_idx'] = upd_df['time_idx'].astype(int)

upd_df.head()

# Replace "." with "_"
upd_df.columns = [col.replace(".", "_") for col in upd_df.columns]

# Define the TimeSeriesDataSet object
# encoder_length = SR * 5
encoder_length = 20
# prediction_length = int(base_len*SR*0.1)
prediction_length = 20
# training_cutoff = int(base_len*SR*0.9) # 80% of the data will be used for training
training_cutoff = upd_df["time_idx"].max() - prediction_length

dataset = TimeSeriesDataSet(
    data=upd_df.loc[lambda x: x.time_idx <= training_cutoff],
    time_idx="time_idx", # index of the time column in the data
    target = "values",
    # categorical_encoders={"group": NaNLabelEncoder().fit(upd_df.group)},
    group_ids= ["group"], # You can group the time series by some columns if needed
    # min_prediction_idx=int(len(upd_df) * 0.9),
    max_encoder_length=encoder_length,
    max_prediction_length=prediction_length,
    time_varying_unknown_reals=["values"],
)

validation = TimeSeriesDataSet.from_dataset(dataset, upd_df, min_prediction_idx=training_cutoff+1)

batch_size = 2

train_dataloader = dataset.to_dataloader(
    train=True, batch_size=batch_size, num_workers=0, batch_sampler="synchronized"
)

val_dataloader = validation.to_dataloader(
    train=False, batch_size=batch_size, num_workers=0, batch_sampler="synchronized"
)

net = DeepAR.from_dataset(
    dataset,
    learning_rate=1e-2,
    hidden_size=10,
    rnn_layers=2,
    optimizer="Adam",
)

early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=1e-4, patience=50, verbose=False, mode="min")
trainer = pl.Trainer(
    max_epochs=10,
    accelerator="cpu",
    enable_model_summary=True,
    gradient_clip_val=0.1,
    limit_train_batches=100,
    callbacks=[early_stop_callback],
    enable_checkpointing=False,
    check_val_every_n_epoch=5
)

trainer.fit(
    net,
    train_dataloaders=train_dataloader,
    val_dataloaders=val_dataloader,
)

from pytorch_forecasting.data.examples import generate_ar_data

data = generate_ar_data(seasonality=10.0, timesteps=400, n_series=100, seed=42)
data["static"] = 2
data["date"] = pd.Timestamp("2020-01-01") + pd.to_timedelta(data.time_idx, "D")
data.head()

data['series'].unique()

from pytorch_forecasting.data.examples import generate_ar_data

data = generate_ar_data(seasonality=10.0, timesteps=400, n_series=100, seed=42)
data["static"] = 2
data["date"] = pd.Timestamp("2020-01-01") + pd.to_timedelta(data.time_idx, "D")

data = data.astype(dict(series=str))

data

"""## **Find the recording lengths by type of story**"""

!pip install mutagen

import mutagen
from mutagen.wave import WAVE

def recording_lengths_plot(files, ax, title=None, data_folder=os.path.join(DRIVE_PATH, 'wav files')):
    lengths = []
    audio_length = {}

    for _, filename in files.items():
        if filename is not None:
            audio = WAVE(os.path.join(data_folder, filename))
            lengths.append(int(audio.info.length))
            audio_length[filename] = int(audio.info.length)
        
    ax.hist(lengths, bins=20)
    if title:
        ax.set_title(title)
    ax.set_xlabel('Duration, seconds')
    return audio_length


fig, ax = plt.subplots(1, 3, sharex=True, figsize=(12, 4))    
narr_length = recording_lengths_plot(df['audio.narrative'], ax[0], title='Narrative')
story_length = recording_lengths_plot(df['audio.story'], ax[1], title='Story')
instr_length = recording_lengths_plot(df['audio.instruction'], ax[2], title='Instruction')
plt.show()



min_narr = min(narr_length, key=narr_length.get)
max_narr = max(narr_length, key=narr_length.get)

min_story = min(story_length, key=story_length.get)
max_story = max(story_length, key=story_length.get)

min_instr = min(instr_length, key=instr_length.get)
max_instr = max(instr_length, key=instr_length.get)

print(f'Narrative:\n\t  Shortest audio: {min_narr}\t Length: {narr_length[min_narr]}\n\t  Longest Audio: {max_narr}\t Length: {narr_length[max_narr]} \n\n' )
print(f'Story:\n\t  Shortest audio: {min_story}\t Length: {story_length[min_story]}\n\t  Longest Audio: {max_story} \t Length: {story_length[max_story]} \n\n' )
print(f'Instruction:\n\t  Shortest audio: {min_instr}\t Length: {instr_length[min_instr]}\n\t  Longest Audio: {max_instr}\t Length: {instr_length[max_instr]}\n' )

np.quantile(list(narr_length.values()), [0.85])

pd.DataFrame(data=list(narr_length.values())).describe()

df_lengths = pd.DataFrame(data=[list(narr_length.values()) + list(story_length.values()) + list(instr_length.values())]).T

df_lengths.describe()

df_lengths.describe().T[['min', '25%', '50%', '75%', 'max']].rename({0: 'duration'}, axis=0)

df_lengths

df_lengths.to_csv('recording_lengths_all.csv')

narr_length.to_csv('recording_lengths_narrative.csv')
story_length.to_csv('recording_lengths_story.csv')
instr_length.to_csv('recording_lengths_instr.csv')

concatenated_df = pd.DataFrame([[i[:6] for i in narr_length.keys()] ,narr_length.values(), story_length.values(), instr_length.values()]).T

concatenated_df.rename({0 : 'id', 1 : 'narrative', 2 : 'story', 3 : 'instruction'}, axis = 1, inplace=True)

concatenated_df.to_csv('recording_lengths_by_type.csv')

"""## **Test audio transformations**"""

signal, sr = lb.load(os.path.join(DRIVE_PATH, 'wav files/PN-019-pic-1-sportsman.wav'), sr=SR)

import IPython.display as ipd

ipd.Audio(signal, rate=sr)

signal_trimmed, index = lb.effects.trim(signal, top_db=40)

signal.shape

signal_trimmed.shape

plt.plot(signal)

plt.plot(signal_trimmed)

# print(lb.get_duration(signal_trimmed))
print(lb.get_duration(signal))

ipd.Audio(signal_trimmed, rate=sr)

signal

y, sr = lb.load(lb.ex('choice'))

y.shape

fast_fourier_transf = np.fft.fft(signal)

magnitude = np.abs(fast_fourier_transf)

frequency = np.linspace(0, sr, len(magnitude))

left_mag = magnitude[:int(len(magnitude)/2)]
left_freq = frequency[:int(len(frequency)/2)]


plt.plot(left_freq, left_mag)
plt.title('Discrete-Fourier Transform', fontdict=dict(size=15))
plt.xlabel('Frequency', fontdict=dict(size=12))
plt.ylabel('Magnitude', fontdict=dict(size=12))
plt.show()

len(fast_fourier_transf)

dur_time = lb.get_duration(signal,sr=sr)
time_sec = round(dur_time)
S = lb.feature.melspectrogram(y=signal, sr=sr, fmax=8000)
S_dB = lb.power_to_db(S, ref=np.max)

start = 0
end = time_sec
time_series_sec = np.linspace(start,end,len(S_dB[0]))

S_dB

len(S_dB[0])

sp_c = lb.feature.spectral_centroid(y = signal, sr = sr, hop_length = 256)

sp_dB = lb.power_to_db(sp_c, ref=np.max)

pd.DataFrame(sp_dB[0])



len(sp_dB[0])

plt.plot(sp_dB[0])
plt.show()

import scipy

a = lb.lpc(signal, order=3)
b = np.hstack([[0], -1 * a[1:]])
y_hat = scipy.signal.lfilter(b, [1], signal)
fig, ax = plt.subplots()
ax.plot(signal)
ax.plot(y_hat, linestyle='--')
ax.legend(['y', 'y_hat'])
ax.set_title('LP Model Forward Prediction')

len(y_hat)

b

# training = TimeSeriesDataSet(
#     df[lambda x: x.time_idx],
#     time_idx="time_idx",
#     target="value",
#     group_ids=["series"],
#     static_categoricals=[
#         "series"
#     ],  # as we plan to forecast correlations, it is important to use series characteristics (e.g. a series identifier)
#     time_varying_unknown_reals=["value"]
# )

# validation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training_cutoff + 1)
# batch_size = 128

# train_dataloader = training.to_dataloader(
#     train=True, batch_size=batch_size, num_workers=0, batch_sampler="synchronized"
# )
# val_dataloader = validation.to_dataloader(
#     train=False, batch_size=batch_size, num_workers=0, batch_sampler="synchronized"
# )

